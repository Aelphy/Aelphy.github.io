[{"authors":["admin"],"categories":null,"content":"I am a PhD student at ETH Zurich in the Photogrammetry and Remote Sensing Lab, supervised by Prof. Dr. Konrad Schindler.\nBefore moving to Switzerland I completed my masters in Moscow where I graduated with honors from Moscow Institute of Physics and Technology (MIPT) and from Skolkovo Institute of Science and Technology (Skoltech)\nI started my expansion towards west with move to Kazan (where I spent 1 year at Innopolis) from Ekaterinburg (where I grew up and made my first bachelor degree in electrical engineering and graduated with honors from Ural Federal University).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at ETH Zurich in the Photogrammetry and Remote Sensing Lab, supervised by Prof. Dr. Konrad Schindler.\nBefore moving to Switzerland I completed my masters in Moscow where I graduated with honors from Moscow Institute of Physics and Technology (MIPT) and from Skolkovo Institute of Science and Technology (Skoltech)\nI started my expansion towards west with move to Kazan (where I spent 1 year at Innopolis) from Ekaterinburg (where I grew up and made my first bachelor degree in electrical engineering and graduated with honors from Ural Federal University).","tags":null,"title":"","type":"authors"},{"authors":["Shengyu Huang","Mikhail Usvyatsov","Konrad Schindler"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"113b03cdfd16c2f2017761cefa13647c","permalink":"/publication/huang-2020-indoor/","publishdate":"2020-02-29T01:18:36.55387Z","relpermalink":"/publication/huang-2020-indoor/","section":"publication","summary":"Recognising in what type of environment one is located is an important perception task. For instance, for a robot operating in indoors it is helpful to be aware whether it is in a kitchen, a hallway or a bedroom. Existing approaches attempt to classify the scene based on 2D images or 2.5D range images. Here, we study scene recognition from 3D point cloud (or voxel) data, and show that it greatly outperforms methods based on 2D birds-eye views. Moreover, we advocate multi-task learning as a way of improving scene recognition, building on the fact that the scene type is highly correlated with the objects in the scene, and therefore with its semantic segmentation into different object classes. In a series of ablation studies, we show that successful scene recognition is not just the recognition of individual objects unique to some scene type (such as a bathtub), but depends on several different cues, including coarse 3D geometry, colour, and the (implicit) distribution of object categories. Moreover, we demonstrate that surprisingly sparse 3D data is sufficient to classify indoor scenes with good accuracy.","tags":null,"title":"Indoor Scene Recognition in 3D","type":"publication"},{"authors":["Mikhail Usvyatsov","Konrad Schindler"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c319fab7656d65c1d8e4028ca51310c0","permalink":"/publication/usvyatsov-2019-visual/","publishdate":"2020-02-29T01:18:36.55387Z","relpermalink":"/publication/usvyatsov-2019-visual/","section":"publication","summary":"Recognising relevant objects or object states in its environment is a basic capability for an autonomous robot. The dominant approach to object recognition in images and range images is classification by supervised machine learning, nowadays mostly with deep convolutional neural networks (CNNs). This works well for target classes whose variability can be completely covered with training examples. However, a robot moving in the wild, i.e., in an environment that is not known at the time the recognition system is trained, will often face \u001bmph{domain shift}: the training data cannot be assumed to exhaustively cover all the within-class variability that will be encountered in the test data. In that situation, learning is in principle possible, since the training set does capture the defining properties, respectively dissimilarities, of the target classes. But directly training a CNN to predict class probabilities is prone to overfitting to irrelevant correlations between the class labels and the specific subset of the target class that is represented in the training set. We explore the idea to instead learn a Siamese CNN that acts as similarity function between pairs of training examples. Class predictions are then obtained by measuring the similarities between a new test instance and the training samples. We show that the CNN embedding correctly recovers the relative similarities to arbitrary class exemplars in the training set. And that therefore few, randomly picked training exemplars are sufficient to achieve good predictions, making the procedure efficient.","tags":null,"title":"Visual recognition in the wild by sampling deep similarity functions","type":"publication"},{"authors":["Timo Hackel","Mikhail Usvyatsov","Silvano Galliani","Jan D Wegner","Konrad Schindler"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"2a801d6f57e604d800ce6c841371d88d","permalink":"/publication/hackel-2018-inference/","publishdate":"2020-02-29T01:18:36.553322Z","relpermalink":"/publication/hackel-2018-inference/","section":"publication","summary":"While CNNs naturally lend themselves to densely sampled data, and sophisticated implementations are available, they lack the ability to efficiently process sparse data. In this work we introduce a suite of tools that exploit sparsity in both the feature maps and the filter weights, and thereby allow for significantly lower memory footprints and computation times than the conventional dense framework when processing data with a high degree of sparsity. Our scheme provides (i) an efficient GPU implementation of a convolution layer based on direct, sparse convolution; (ii) a filter step within the convolution layer, which we call attention, that prevents fill-in, i.e., the tendency of convolution to rapidly decrease sparsity, and guarantees an upper bound on the computational resources; and (iii) an adaptation of the back-propagation algorithm, which makes it possible to combine our approach with standard learning frameworks, while still exploiting sparsity in the data and the model.","tags":null,"title":"Inference, learning and attention mechanisms that exploit and preserve sparsity in convolutional networks","type":"publication"},{"authors":["Maxim Borisyak","Mikhail Usvyatsov","Michael Mulhearn","Chase Shimmin","Andrey Ustyuzhanin"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"2b883d6927bfb343ecb54d9784abf3a6","permalink":"/publication/borisyak-2017-muon/","publishdate":"2020-02-29T01:18:36.552293Z","relpermalink":"/publication/borisyak-2017-muon/","section":"publication","summary":"Neural architecture that allows for simultaneous optimization of computational cost with per-pixel cross-entropy loss.","tags":null,"title":"Muon trigger for mobile phones","type":"publication"}]